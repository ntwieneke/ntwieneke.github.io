{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TweetGroups\n",
    "\n",
    "Twitter is an integral part of marketing and can’t be ignored.  Twitter interactions can not only be a good metric for tracking a marketing campaign’s performance, but it can also be the cause of product or brands success and failure.\n",
    "\n",
    "In recent years we have all seen examples of bad tweets that have ruined reputations and tarnished brands, so making sure that your company's tweets are throughly planned is essential.  The process of creating and maintaining an effective presence on Twitter is a complex one, but TweetGroups can help you get started:\n",
    "\n",
    "#### TweetGroups helps to answer two fundamental marketing questions:\n",
    "1.  What are our market segements (groups of customers)?\n",
    "2.  How do we engage these customers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TweetGroups uses the Twitter Search api to return a series of tweets that contain a specific query term (in this example we will use 'Go Pro').  Once the tweets are loaded, TweetGroups clusters all hashtags found in these Tweets via the text of the Tweets that contain them.  In order to do this we will need to follow a few pre-processing steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-processing with spaCy\n",
    "\n",
    "For text processing step we chose to use the Natural Language Processing library spaCy due to a few advantages over other libraries:\n",
    "\n",
    "1.  Performance: spaCy is written in Cython and contains a wide array of NLP functions that can be parallelized and execute quickly\n",
    "\n",
    "2.  Flexability: spaCy is an object-oriented approach, so the returned tokens have attributes that are particularly useful for social media data, including the ability to tokenize emojis, parts of speech tagging, and named entity recognition\n",
    "\n",
    "3.  Usability: spaCy code is clear, concise, well-documented and actively supported\n",
    "\n",
    "Let's show a quick performance comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import spacy\n",
    "import timeit\n",
    "import textacy\n",
    "%timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading a pickled list of tweets\n",
    "import pandas as pd\n",
    "tweet_list = pd.read_pickle('/Users/nathanbackblaze/DS/Metis/projects/Final Projects/tweet_list_gopro.pkl')\n",
    "# this list contains about 14000 tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy's various Parts of Speech attributes can be very helpful when parsing Tweets.  One example of this is that you can use spacy to filter tweets where your company / product name is not the subject of the tweet.  In the example bellow I'll demonstrate how spaCy can help filter out tweets in which 'Uber' is used as a verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 16.2 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# loading the spacy module\n",
    "nlp = spacy.en.English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Spacy English parser does take some time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a fake set of tweets to demonstrate this feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo_tweets = [u'I took an Uber to the other side of town',u'Uber is now launching new features soon',u'Let us uber over to the other side of town']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_show_attributes(string):\n",
    "    parsed = nlp(string)\n",
    "    for token in parsed:\n",
    "#   this next line of code will print each word's original form, part of speech tag, and the dependants\n",
    "        print(token.orth_, token.dep_, token.head.orth_, [t.orth_ for t in token.lefts], [t.orth_ for t in token.rights])\n",
    "    print\n",
    "    print \"<next tweet>\"\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'I', u'nsubj', u'took', [], [])\n",
      "(u'took', u'ROOT', u'took', [u'I'], [u'Uber', u'to'])\n",
      "(u'an', u'det', u'Uber', [], [])\n",
      "(u'Uber', u'dobj', u'took', [u'an'], [])\n",
      "(u'to', u'prep', u'took', [], [u'side'])\n",
      "(u'the', u'det', u'side', [], [])\n",
      "(u'other', u'amod', u'side', [], [])\n",
      "(u'side', u'pobj', u'to', [u'the', u'other'], [u'of'])\n",
      "(u'of', u'prep', u'side', [], [u'town'])\n",
      "(u'town', u'pobj', u'of', [], [])\n",
      "\n",
      "<next tweet>\n",
      "\n",
      "(u'Uber', u'nsubj', u'launching', [], [])\n",
      "(u'is', u'aux', u'launching', [], [])\n",
      "(u'now', u'advmod', u'launching', [], [])\n",
      "(u'launching', u'ROOT', u'launching', [u'Uber', u'is', u'now'], [u'features', u'soon'])\n",
      "(u'new', u'amod', u'features', [], [])\n",
      "(u'features', u'dobj', u'launching', [u'new'], [])\n",
      "(u'soon', u'advmod', u'launching', [], [])\n",
      "\n",
      "<next tweet>\n",
      "\n",
      "(u'Let', u'ROOT', u'Let', [], [u'uber'])\n",
      "(u'us', u'nsubj', u'uber', [], [])\n",
      "(u'uber', u'ccomp', u'Let', [u'us'], [u'over', u'to'])\n",
      "(u'over', u'prt', u'uber', [], [])\n",
      "(u'to', u'prep', u'uber', [], [u'side'])\n",
      "(u'the', u'det', u'side', [], [])\n",
      "(u'other', u'amod', u'side', [], [])\n",
      "(u'side', u'pobj', u'to', [u'the', u'other'], [u'of'])\n",
      "(u'of', u'prep', u'side', [], [u'town'])\n",
      "(u'town', u'pobj', u'of', [], [])\n",
      "\n",
      "<next tweet>\n",
      "\n",
      "(u'I', u'nsubj', u'took', [], [])\n",
      "(u'took', u'ROOT', u'took', [u'I'], [u'Uber', u'to'])\n",
      "(u'an', u'det', u'Uber', [], [])\n",
      "(u'Uber', u'dobj', u'took', [u'an'], [])\n",
      "(u'to', u'prep', u'took', [], [u'side'])\n",
      "(u'the', u'det', u'side', [], [])\n",
      "(u'other', u'amod', u'side', [], [])\n",
      "(u'side', u'pobj', u'to', [u'the', u'other'], [u'of'])\n",
      "(u'of', u'prep', u'side', [], [u'town'])\n",
      "(u'town', u'pobj', u'of', [], [])\n",
      "\n",
      "<next tweet>\n",
      "\n",
      "(u'Uber', u'nsubj', u'launching', [], [])\n",
      "(u'is', u'aux', u'launching', [], [])\n",
      "(u'now', u'advmod', u'launching', [], [])\n",
      "(u'launching', u'ROOT', u'launching', [u'Uber', u'is', u'now'], [u'features', u'soon'])\n",
      "(u'new', u'amod', u'features', [], [])\n",
      "(u'features', u'dobj', u'launching', [u'new'], [])\n",
      "(u'soon', u'advmod', u'launching', [], [])\n",
      "\n",
      "<next tweet>\n",
      "\n",
      "(u'Let', u'ROOT', u'Let', [], [u'uber'])\n",
      "(u'us', u'nsubj', u'uber', [], [])\n",
      "(u'uber', u'ccomp', u'Let', [u'us'], [u'over', u'to'])\n",
      "(u'over', u'prt', u'uber', [], [])\n",
      "(u'to', u'prep', u'uber', [], [u'side'])\n",
      "(u'the', u'det', u'side', [], [])\n",
      "(u'other', u'amod', u'side', [], [])\n",
      "(u'side', u'pobj', u'to', [u'the', u'other'], [u'of'])\n",
      "(u'of', u'prep', u'side', [], [u'town'])\n",
      "(u'town', u'pobj', u'of', [], [])\n",
      "\n",
      "<next tweet>\n",
      "\n",
      "(u'I', u'nsubj', u'took', [], [])\n",
      "(u'took', u'ROOT', u'took', [u'I'], [u'Uber', u'to'])\n",
      "(u'an', u'det', u'Uber', [], [])\n",
      "(u'Uber', u'dobj', u'took', [u'an'], [])\n",
      "(u'to', u'prep', u'took', [], [u'side'])\n",
      "(u'the', u'det', u'side', [], [])\n",
      "(u'other', u'amod', u'side', [], [])\n",
      "(u'side', u'pobj', u'to', [u'the', u'other'], [u'of'])\n",
      "(u'of', u'prep', u'side', [], [u'town'])\n",
      "(u'town', u'pobj', u'of', [], [])\n",
      "\n",
      "<next tweet>\n",
      "\n",
      "(u'Uber', u'nsubj', u'launching', [], [])\n",
      "(u'is', u'aux', u'launching', [], [])\n",
      "(u'now', u'advmod', u'launching', [], [])\n",
      "(u'launching', u'ROOT', u'launching', [u'Uber', u'is', u'now'], [u'features', u'soon'])\n",
      "(u'new', u'amod', u'features', [], [])\n",
      "(u'features', u'dobj', u'launching', [u'new'], [])\n",
      "(u'soon', u'advmod', u'launching', [], [])\n",
      "\n",
      "<next tweet>\n",
      "\n",
      "(u'Let', u'ROOT', u'Let', [], [u'uber'])\n",
      "(u'us', u'nsubj', u'uber', [], [])\n",
      "(u'uber', u'ccomp', u'Let', [u'us'], [u'over', u'to'])\n",
      "(u'over', u'prt', u'uber', [], [])\n",
      "(u'to', u'prep', u'uber', [], [u'side'])\n",
      "(u'the', u'det', u'side', [], [])\n",
      "(u'other', u'amod', u'side', [], [])\n",
      "(u'side', u'pobj', u'to', [u'the', u'other'], [u'of'])\n",
      "(u'of', u'prep', u'side', [], [u'town'])\n",
      "(u'town', u'pobj', u'of', [], [])\n",
      "\n",
      "<next tweet>\n",
      "\n",
      "(u'I', u'nsubj', u'took', [], [])\n",
      "(u'took', u'ROOT', u'took', [u'I'], [u'Uber', u'to'])\n",
      "(u'an', u'det', u'Uber', [], [])\n",
      "(u'Uber', u'dobj', u'took', [u'an'], [])\n",
      "(u'to', u'prep', u'took', [], [u'side'])\n",
      "(u'the', u'det', u'side', [], [])\n",
      "(u'other', u'amod', u'side', [], [])\n",
      "(u'side', u'pobj', u'to', [u'the', u'other'], [u'of'])\n",
      "(u'of', u'prep', u'side', [], [u'town'])\n",
      "(u'town', u'pobj', u'of', [], [])\n",
      "\n",
      "<next tweet>\n",
      "\n",
      "(u'Uber', u'nsubj', u'launching', [], [])\n",
      "(u'is', u'aux', u'launching', [], [])\n",
      "(u'now', u'advmod', u'launching', [], [])\n",
      "(u'launching', u'ROOT', u'launching', [u'Uber', u'is', u'now'], [u'features', u'soon'])\n",
      "(u'new', u'amod', u'features', [], [])\n",
      "(u'features', u'dobj', u'launching', [u'new'], [])\n",
      "(u'soon', u'advmod', u'launching', [], [])\n",
      "\n",
      "<next tweet>\n",
      "\n",
      "(u'Let', u'ROOT', u'Let', [], [u'uber'])\n",
      "(u'us', u'nsubj', u'uber', [], [])\n",
      "(u'uber', u'ccomp', u'Let', [u'us'], [u'over', u'to'])\n",
      "(u'over', u'prt', u'uber', [], [])\n",
      "(u'to', u'prep', u'uber', [], [u'side'])\n",
      "(u'the', u'det', u'side', [], [])\n",
      "(u'other', u'amod', u'side', [], [])\n",
      "(u'side', u'pobj', u'to', [u'the', u'other'], [u'of'])\n",
      "(u'of', u'prep', u'side', [], [u'town'])\n",
      "(u'town', u'pobj', u'of', [], [])\n",
      "\n",
      "<next tweet>\n",
      "\n",
      "The slowest run took 132.46 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1 loop, best of 3: 2.18 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for tweet in demo_tweets:\n",
    "    parse_show_attributes(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above example we can see that spaCy was able to identify:\n",
    "- Uber was the object in the 1st example\n",
    "- Uber was the subject in the 2nd example\n",
    "- Uber was a clause in the 3rd example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might be helpful to filter out noise in an instance where you are trying to determine what users are saying about your firm's actions rather than how they utilize the product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note this link helped me translate the part of speech tags http://nlp.stanford.edu/software/dependencies_manual.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also detect other named entities in a Tweet, this might be helpful if you wish to know what other products / persons are mentioned in a Tweet (a feature a I would like to add to my project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo_tweet = [u'I took an Uber in San Francisco on Monday; it was a Tesla and Elon Musk was there.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uber GPE\n",
      "San Francisco GPE\n",
      "Monday DATE\n",
      "Tesla GPE\n",
      "Elon Musk PERSON\n"
     ]
    }
   ],
   "source": [
    "entities = list(nlp(unicode(demo_tweet)).ents)\n",
    "for entity in entities:\n",
    "    print entity.orth_,entity.label_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature could allow you to map out all the other entities (people / companies) that are mentioned in your tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy also allows you to handle special tokens such as emojis or internet slang, which are extremely relevant for tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol NOUN\n",
      "i PRON\n",
      "luv VERB\n",
      "Uber PROPN\n",
      ":) PUNCT\n"
     ]
    }
   ],
   "source": [
    "slang_tweet = u\"lol i luv Uber :)\"\n",
    "for token in nlp(slang_tweet):\n",
    "    print token.orth_, token.pos_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has a wide array of potential applications, for instance you could use emoji tokens to help with sentiment analysis (since emojis can be clear indicators of sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK vs. spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show a side-by-side comparison of NLTK vs spaCy , first we will only tokenize the words in the list of tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 3.29 s per loop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TimeitResult : 1 loop, best of 3: 3.29 s per loop>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timeit\n",
    "for string in tweet_list:\n",
    "    nltk.tokenize.word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1.81 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for string in tweet_list:\n",
    "    nlp(string,tag=False,parse=False,entity=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare the much more complex parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 22.3 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for string in tweet_list:\n",
    "    nltk.pos_tag(nltk.tokenize.word_tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 2.49 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for string in tweet_list:\n",
    "    nlp(string,tag=True,parse=False,entity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 85.8 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for string in tweet_list:\n",
    "    textblob.TextBlob(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy also allows for multi-threading to help speed up bigger jobs when you have more computational resources available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 12.9 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for doc in nlp.pipe(tweet_list, n_threads = 1, batch_size = 50):\n",
    "    assert doc.is_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 11.8 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for doc in nlp.pipe(tweet_list, n_threads = 8, batch_size = 50):\n",
    "    assert doc.is_parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference > http://blog.thedataincubator.com/2016/04/nltk-vs-spacy-natural-language-processing-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textacy is package built on top of spacy that easily integrates with TFIDF and dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextCorpus(2510 docs; 232695 tokens)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = textacy.TextCorpus.from_texts('en',tweet_list)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Corpus' variable here is a Textacy object not only has the word tokens from our documents, but also has a suite of other attributes, including part of speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our tokenized text matrix, we need to properly weight unimportant words.  Term Frequency Inverse Document Frequency will allow us automatically down-weight words that have less significance in determining the document's topic.\n",
    "\n",
    "This is especially cruicial in our case.  Because we are taking tweets that match a certain query term ('GoPro'), this means that many of these Tweets will contain the same words such as the query term itself.\n",
    "\n",
    "In the following step, we are also specifiying that we are tokenizing ngrams, or pairs and triplets of words.  This allows us to tokenize more specific semantic behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_term_matrix, id2term = corpus.as_doc_term_matrix((doc.as_terms_list(words=True, ngrams=(2,3), named_entities=True)for doc in corpus),weighting='tfidf', normalize=True, smooth_idf=True, min_df=2, max_df=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a performance demo, here is a quick end to end script that reads the raw text, parses weighted tokens, and outputs the topic models, which might take considerable time with other tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 16.7 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "corpus = textacy.TextCorpus.from_texts('en',tweet_list)\n",
    "doc_term_matrix, id2term = corpus.as_doc_term_matrix((doc.as_terms_list(words=True, ngrams=(2,3), named_entities=True)for doc in corpus),weighting='tfidf', normalize=True, smooth_idf=True, min_df=2, max_df=0.95)\n",
    "model = textacy.tm.TopicModel('lsa', n_topics=10)\n",
    "model.fit(doc_term_matrix)\n",
    "doc_topic_matrix = model.transform(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is Dimensionality Reduction Important?\n",
    "\n",
    "The text written on social media can be random, arbitrary, and have a wide variety of tokens (including words/phrases/emojis).  Without a way to reduce these high-dimensional token matricies, you can run in to performance issues and clustering may be difficult.  This is particularly import in our case, where we are using clustering algorthims that do not need a n_topics paraments (ie. Affinity Progpogation, DBSCAN, Mean Shift, etc.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (using Singular Value Decomposition)\n",
    "LSA is a simple and fast approach to dimensionality reduction.  It works to 'flatten' the dimensionality of the matrix while retaining as much variance as possible, and can work well with unbalanced topics.\n",
    "\n",
    "LSA does have a few shortcomings, however.  LSA can have trouble in cases where slight variance in words signify a different topic.  For instance, a colleague of mine (Emily) recently built an NLP model using Supreme Court data.  Most of these documents contained a large amount of similar legal jargon, and LSA was unable to effectively parse the latent topics.\n",
    "\n",
    "Textacy makes switching between dimensionality reduction methods easy, as you are simply passing in the chosen method as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2510, 10)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = textacy.tm.TopicModel('lsa', n_topics=10)\n",
    "model.fit(doc_term_matrix)\n",
    "doc_topic_matrix = model.transform(doc_term_matrix)\n",
    "doc_topic_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned LSA (unlike LDA) values are not necessarily human readable, and don't represent a clear topic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05693241, -0.02133441, -0.02307617, -0.02781793,  0.00016982,\n",
       "       -0.00223301, -0.01919703,  0.02144858,  0.00428661, -0.04453258])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('topic', 0, ':', u'gopro   $   11.99   camera   video   hero4   rt   gopro $   @gopro   hero')\n",
      "('topic', 1, ':', u'11.99   $   gopro $   $ 11.99   gopro $ 11.99   dog   13.99   harness   fetch   pet')\n",
      "('topic', 2, ':', u'hero4   camera   gopro hero4   silver   hero4 silver   gopro hero4 silver   edition   244.00   hero   gopro hero')\n",
      "('topic', 3, ':', u'scuba   diving   diva   padi   hawaii   sea   mar   underwater   scubadiving   gopro hero4')\n",
      "('topic', 4, ':', u'selfie   @gopro   @xgame @gopro   ion   deftfam   turnup   turnuptuesday   @xgame   texas   austin')\n",
      "('topic', 5, ':', u'drone   quad   fpv   13.99   dji   fly   airquad   atlantic\\u2026   naza   turnigy')\n",
      "('topic', 6, ':', u'13.99   $ 13.99   gopro $ 13.99   clamp   flex   adjustable   great   adjustable neck   mount with adjustable   neck')\n",
      "('topic', 7, ':', u'socialmedia   videoshoot   magmabags   denondj   editing   landrover   djlife   route   countryside   dj')\n",
      "('topic', 8, ':', u'gig   download   https://t.co/5zikmckbox   uk   vlog   montage   socialmedia   editing   route   denondj')\n",
      "('topic', 9, ':', u'drone   quad   socialmedia   denondj   editing   magmabags   videoshoot   route   countryside   dj')\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, top_terms in model.top_topic_terms(id2term, top_n=10):\n",
    "    print('topic', topic_idx, ':', '   '.join(top_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirchlet Allocation\n",
    "Pronounced 'deer_uh_shlay', this method assigns a probability that the document belongs to a given topic making the results more human readable.  LDA is iterative, so it begins by randomly assigning topical distributions and iterates through to optimize the assignments, so it will generally perform slower than LSA.\n",
    "\n",
    "LDA also has a few limitions.  When using LDA you assume a Dirchlet Prior, which stipulates that the original text documents are written in  the following manner.\n",
    "- You first decide a set number of words to that the document will have\n",
    "- You decide on a mixture of topics and draw words from those topics\n",
    "- The words you use are chosen from each topic 'corpus'\n",
    "\n",
    "As you can see, this assumption does not necessarily mesh well with how Tweets are actually written.  Tweets are approximately all of the same length (~ 140 characters), and you may only really be Tweeting about one topic, not choosing from a distribution.\n",
    "\n",
    "Furthermore, LDA is designed to find topics that are 'furthest' from each other, so LDA can have trouble identifying uneven topic distributions (which is often the case on Twitter).\n",
    "\n",
    "As a result, LDA is often better suited for understanding the distribution of a topics in the given corpus, rather than specifically classifying each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA requires and interger matrix, so TFIDF cannot be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "doc_term_matrix_tf, id2term = corpus.as_doc_term_matrix((doc.as_terms_list(words=True, ngrams=(2,3), named_entities=True)for doc in corpus),weighting='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2510x37158 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 102213 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2510, 10)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timeit\n",
    "model = textacy.tm.TopicModel('lda', n_topics=10)\n",
    "model.fit(doc_term_matrix_tf)\n",
    "doc_topic_matrix_tf = model.transform(doc_term_matrix_tf)\n",
    "doc_topic_matrix_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10000046,  0.1000001 ,  0.10000051,  0.10000011,  0.10000051,\n",
       "        0.10000021,  0.10000032,  0.10000075,  3.97297869,  0.10000169])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_matrix_tf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('topic', 0, ':', u'gopro   13.99   morning   $ 13.99   $   gopro $ 13.99   gopro $   happy   travel   rt')\n",
      "('topic', 1, ':', u'gopro   3dprint   give   \\U0001f440   blackdress   link   cape   littleblackridinghood   pullup   vintage')\n",
      "('topic', 2, ':', u'gopro   2016   vscocam   vsco   brasil   camera   care   panda   studio   landrover')\n",
      "('topic', 3, ':', u'gopro   europe   fly   agario   turkey   fake   slitherio   turkiye   afk   oyun')\n",
      "('topic', 4, ':', u'gopro   @gopro   drone   fun   rt   quad   video   parkour   freerunning   @goprouk')\n",
      "('topic', 5, ':', u'gopro   summer   travel   thailand   incredible   video   photography   blogger   leh   ladakh')\n",
      "('topic', 6, ':', u'gopro   travel   gopro\\u306e\\u3042\\u308b\\u751f\\u6d3b   @gopro   oahu   nassau   dallas   goprohero4   hawaii   calpe')\n",
      "('topic', 7, ':', u'gopro   venice   munich   snorkel   best   today   \\u6c96\\u7e04   edit   dji   best sale')\n",
      "('topic', 8, ':', u'gopro   rt   video   goprohero4   hero4   @gopro   camera   gopro\\u2026   hero   summer')\n",
      "('topic', 9, ':', u'gopro   selfie   \\u5bcc\\u58eb\\u5c71   @gopro   download   2016   music   gig   vlog   lens')\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, top_terms in model.top_topic_terms(id2term, top_n=10):\n",
    "    print('topic', topic_idx, ':', '   '.join(top_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Negative Matrix Factorization (NMF)\n",
    "True to it's name, this newer method requires that the given matrix be non-negative.  While similar to LDA, NMF has more restrictive paramenters.  This leads to less flexibility, but also allows for an improvement of performance over LDA and works well out of the box on short texts (like tweets).\n",
    "\n",
    "One noted downside of NMF is that results can vary wildly with a slight change of n_topics.  While the topic distribution may not change significantly with LDA when the n_topics are increased, the NMF topic distributio may go from focused to incoherent.  This is particularily an issue with the clustering methods we will be using as they may have a large number of topics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_term_matrix, id2term = corpus.as_doc_term_matrix((doc.as_terms_list(words=True, ngrams=(2,3), named_entities=True)for doc in corpus),weighting='tfidf', normalize=True, smooth_idf=True, min_df=2, max_df=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 264 ms per loop\n",
      "100 loops, best of 3: 7.95 ms per loop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2510, 10)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%timeit\n",
    "model = textacy.tm.TopicModel('nmf', n_topics=10)\n",
    "model.fit(doc_term_matrix)\n",
    "doc_topic_matrix = model.transform(doc_term_matrix)\n",
    "doc_topic_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05693241, -0.02133441, -0.02307617, -0.02781793,  0.00016982,\n",
       "       -0.00223301, -0.01919703,  0.02144858,  0.00428661, -0.04453258])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('topic', 0, ':', u'gopro   rt   @gopro   video   hero   camera   win   gopro hero   win a gopro   enter')\n",
      "('topic', 1, ':', u'hero4   gopro hero4   silver   $   hero4 silver   gopro hero4 silver   244.00   244   edition   camera')\n",
      "('topic', 2, ':', u'11.99   gopro $ 11.99   $ 11.99   gopro $   dog   $   harness   fetch   pet   pet chest')\n",
      "('topic', 3, ':', u'scuba   diving   diva   padi   hawaii   sea   mar   underwater   scubadiving   buceo')\n",
      "('topic', 4, ':', u'goprooftheday\\u2026   follzqome   likesforlikes   fothograpy   goprohero3   follow   like   gopro   share   climbing')\n",
      "('topic', 5, ':', u'ladakh #   https://t.co/kdtqqyx2cx   leh   blogger   india   ladakh   people   incredible   blog   place')\n",
      "('topic', 6, ':', u'gig   download   https://t.co/5zikmckbox   uk   vlog   montage   vlogs   heavymetal   download gopro montage   @gopro @goprouk https://t.co/5zikmckbox')\n",
      "('topic', 7, ':', u'artfood\\U0001f3a8   \\u6df1\\u5733   shenzhen   luohu   \\u5e7f\\u4e1c\\u2026   \\u7f57\\u6e56   guangdong   party   https://t.co/idcpnh9kec   \\u5e7f\\u4e1c\\u2026 https://t.co/idcpnh9kec')\n",
      "('topic', 8, ':', u'13.99   $ 13.99   gopro $ 13.99   gopro $   $   clamp   flex   adjustable   mount   great')\n",
      "('topic', 9, ':', u'\\u5bcc\\u58eb\\u5c71   \\u7a7a\\u64ae   \\u6cb3\\u53e3\\u6e56   https://t.co/gxgfe547yv   gopro https://t.co/gxgfe547yv   \\u672c\\u6816\\u6e56   \\u5bcc\\u58eb\\u5c71 https://t.co/dyase8h0me   interpose+   \\u4e16\\u754c\\u907a\\u7523 \\u5bcc\\u58eb\\u5c71   https://t.co/dyase8h0me')\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, top_terms in model.top_topic_terms(id2term, top_n=10):\n",
    "    print('topic', topic_idx, ':', '   '.join(top_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
